{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(1, 100)\n",
    "        self.activ = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(100, 1)\n",
    "        self.do = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.activ(x)\n",
    "        x = self.do(x)\n",
    "        x = self.layer2(x)\n",
    "                        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (torch.rand(1000)-0.4)*3\n",
    "y = x**3 + torch.randn(1)*0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "model.train() \n",
    "optim = torch.optim.Adam(model.parameters())\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:529: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 999, loss: 0.05419119447469711\n",
      "epoch: 1, step: 999, loss: 0.0014200137229636312\n",
      "epoch: 2, step: 999, loss: 0.006938278209418058\n",
      "epoch: 3, step: 999, loss: 0.006856575608253479\n",
      "epoch: 4, step: 999, loss: 0.00036289350828155875\n",
      "epoch: 5, step: 999, loss: 0.0002590734220575541\n",
      "epoch: 6, step: 999, loss: 0.005557742901146412\n",
      "epoch: 7, step: 999, loss: 0.001379823312163353\n",
      "epoch: 8, step: 999, loss: 0.002425295999273658\n",
      "epoch: 9, step: 999, loss: 0.0037736091762781143\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (val, t) in enumerate(zip(x, y)):\n",
    "        optim.zero_grad()\n",
    "        predict = model(val.unsqueeze(dim=0))\n",
    "        loss = loss_func(predict, t)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    print(f'epoch: {epoch}, step: {i}, loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    def __init__(self, n_inp, n_out, activation='sigmoid'):\n",
    "        self.w = np.random.randn(n_out, n_inp) * 0.1\n",
    "        self.b = np.random.randn(n_out, 1) * 0.1\n",
    "        if activation == 'sigmoid':\n",
    "            self.activ = sigmoid\n",
    "        if activation == 'relu':\n",
    "            self.activ = relu\n",
    "        elif activation == 'None':\n",
    "            self.activ = None\n",
    "        else:\n",
    "            raise Exception(f'Unknown activation \"{activation}\"')\n",
    "        self._clear_state()\n",
    "\n",
    "    def _clear_state(self):\n",
    "        self.lin = None\n",
    "        self.inp = None\n",
    "        self.d_w = None\n",
    "        self.d_b = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.inp = x\n",
    "        self.lin = np.dot(self.w, x) + self.b\n",
    "        activ = self.activ(self.lin) if self.activ is not None else self.lin\n",
    "\n",
    "        return activ\n",
    "\n",
    "    def backward(self, grad): # grad = d L / d z    Dout \n",
    "        # grad * dz / d lin\n",
    "        if self.activ == sigmoid:\n",
    "            grad_lin = sigmoid_backward(grad, self.lin) \n",
    "        elif self.activ == relu:\n",
    "            grad_lin = relu_backward(grad, self.lin)\n",
    "        else:\n",
    "            grad_lin = grad\n",
    "        # grad_lin * d lin / d w \n",
    "        m = self.inp.shape[1]\n",
    "        self.d_w = np.dot(grad_lin, self.inp.T) / m    # d_in dOut\n",
    "        # grad_lin * d lin / d b \n",
    "        self.d_b = np.sum(grad_lin, axis=1, keepdims=True) / m\n",
    "\n",
    "        grad = np.dot(self.w.T, grad_lin)\n",
    "\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDMomentum1:\n",
    "    def __init__(self, model: LinearLayer, lr=0.001, momentum=0.99):\n",
    "        self.lr = lr\n",
    "        self.m = momentum\n",
    "        self.model = model\n",
    "\n",
    "        self.vel_w = np.zeros_like(model.w)\n",
    "        self.vel_b = np.zeros_like(model.b)\n",
    "\n",
    "    def step(self):\n",
    "        self.vel_w = self.m * self.vel_w - self.lr * self.model.d_w\n",
    "        self.vel_b = self.m * self.vel_b - self.lr * self.model.d_b\n",
    "\n",
    "        self.model.w += self.vel_w\n",
    "        self.model.b += self.vel_b\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.model.d_w = np.zeros_like(self.model.d_w)\n",
    "        self.model.d_b = np.zeros_like(self.model.d_b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptim():\n",
    "    def __init__(self, eta=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.m_dw, self.v_dw = 0, 0\n",
    "        self.m_db, self.v_db = 0, 0\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.eta = eta\n",
    "    def update(self, t, w, b, dw, db):\n",
    "        ## dw, db из текущей минибатча\n",
    "        ## momentum beta 1\n",
    "        # *** веса *** #\n",
    "        self.m_dw = self.beta1*self.m_dw + (1-self.beta1)*dw\n",
    "        # *** смещения *** #\n",
    "        self.m_db = self.beta1*self.m_db + (1-self.beta1)*db\n",
    "\n",
    "        ## rms beta 2\n",
    "        # *** веса *** #\n",
    "        self.v_dw = self.beta2*self.v_dw + (1-self.beta2)*(dw**2)\n",
    "        # *** смещения *** #\n",
    "        self.v_db = self.beta2*self.v_db + (1-self.beta2)*(db)\n",
    "\n",
    "        ## коррекция смещения\n",
    "        m_dw_corr = self.m_dw/(1-self.beta1**t)\n",
    "        m_db_corr = self.m_db/(1-self.beta1**t)\n",
    "        v_dw_corr = self.v_dw/(1-self.beta2**t)\n",
    "        v_db_corr = self.v_db/(1-self.beta2**t)\n",
    "\n",
    "        ## обновить веса и смещения\n",
    "        w = w - self.eta*(m_dw_corr/(np.sqrt(v_dw_corr)+self.epsilon))\n",
    "        b = b - self.eta*(m_db_corr/(np.sqrt(v_db_corr)+self.epsilon))\n",
    "        return w, b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение квадратного уравнения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class QuadraticEquationDataset(Dataset):\n",
    "    def __init__(self, num_samples):\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        a = torch.randn(1)\n",
    "        b = torch.randn(1)\n",
    "        c = torch.randn(1)\n",
    "\n",
    "        D = b**2 - 4*a*c\n",
    "        if D < 0:\n",
    "            x1 = float('nan')\n",
    "            x2 = float('nan')\n",
    "        else:\n",
    "            x1 = (-b + torch.sqrt(D)) / (2*a)\n",
    "            x2 = (-b - torch.sqrt(D)) / (2*a)\n",
    "\n",
    "        return torch.Tensor([a, b, c]), torch.Tensor([x1, x2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = QuadraticEquationDataset(num_samples=1000)\n",
    "train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                          #batch_size=32,\n",
    "                                          shuffle=True, \n",
    "                                          num_workers=2, \n",
    "                                          )\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                          #batch_size=32,\n",
    "                                          shuffle=True, \n",
    "                                          num_workers=2, \n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadraticEquationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuadraticEquationModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(3, 16)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(16, 8)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(8, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QuadraticEquationModel()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "epoch: 0, step: 7, loss: nan\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "epoch: 1, step: 7, loss: nan\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "epoch: 2, step: 7, loss: nan\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "epoch: 3, step: 7, loss: nan\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "epoch: 4, step: 7, loss: nan\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "epoch: 5, step: 7, loss: nan\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "epoch: 6, step: 7, loss: nan\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "epoch: 7, step: 7, loss: nan\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "epoch: 8, step: 7, loss: nan\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n",
      "epoch: 9, step: 7, loss: nan\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, t) in enumerate(trainloader, 0):\n",
    "        optim.zero_grad()\n",
    "        predict = model(inputs)\n",
    "        print(predict)\n",
    "        loss = loss_func(predict, t)\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    print(f'epoch: {epoch}, step: {i}, loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[ 0.5180, -0.8559]])\n",
      "tensor([[nan, nan]]) tensor([[ 6.2646, -0.0524]])\n",
      "tensor([[nan, nan]]) tensor([[-84.3136,   2.8203]])\n",
      "tensor([[nan, nan]]) tensor([[  0.1294, -14.5525]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[ 1.5542, -1.0139]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[ 1.2038, -1.1941]])\n",
      "tensor([[nan, nan]]) tensor([[ 1.2616, -0.5858]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[ 0.3271, -1.4031]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[-1.1750,  1.3107]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[0.0014, 1.1258]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[-0.0892,  4.2295]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[-6.8424,  0.1574]])\n",
      "tensor([[nan, nan]]) tensor([[-1.1956,  1.8020]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[ 0.8049, -1.9923]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[-1.4751,  0.3403]])\n",
      "tensor([[nan, nan]]) tensor([[ 1.0764, -0.6247]])\n",
      "tensor([[nan, nan]]) tensor([[ 0.1697, -0.4951]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[-0.7511,  2.5754]])\n",
      "tensor([[nan, nan]]) tensor([[-0.9520,  2.2289]])\n",
      "tensor([[nan, nan]]) tensor([[ 0.7254, 17.3496]])\n",
      "tensor([[nan, nan]]) tensor([[-0.7044,  4.8074]])\n",
      "tensor([[nan, nan]]) tensor([[-1.0674,  1.2571]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[ 0.6670, -1.2645]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[-1.0555,  0.1317]])\n",
      "tensor([[nan, nan]]) tensor([[-5.3939,  0.6541]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[-1.1341, -2.5289]])\n",
      "tensor([[nan, nan]]) tensor([[-0.3091, -0.9697]])\n",
      "tensor([[nan, nan]]) tensor([[ 0.9709, -1.0890]])\n",
      "tensor([[nan, nan]]) tensor([[-0.4921,  0.8475]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[-0.2235,  1.6299]])\n",
      "tensor([[nan, nan]]) tensor([[0.4112, 0.8707]])\n",
      "tensor([[nan, nan]]) tensor([[39.8755, -0.3118]])\n",
      "tensor([[nan, nan]]) tensor([[-1.7807,  0.4562]])\n",
      "tensor([[nan, nan]]) tensor([[ 0.4252, -0.4327]])\n",
      "tensor([[nan, nan]]) tensor([[3.7021, 0.0385]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[ 1.6897, -0.3395]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[-0.5586,  0.5884]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[ 2.8420, -3.6741]])\n",
      "tensor([[nan, nan]]) tensor([[ 0.0560, -0.1075]])\n",
      "tensor([[nan, nan]]) tensor([[-2.5236,  0.9026]])\n",
      "tensor([[nan, nan]]) tensor([[ 0.4394, -1.5961]])\n",
      "tensor([[nan, nan]]) tensor([[-0.2793,  9.4213]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[19.3133, -0.7127]])\n",
      "tensor([[nan, nan]]) tensor([[-0.0256, -0.9067]])\n",
      "tensor([[nan, nan]]) tensor([[-1.0343,  1.6700]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[0.0072, 0.7584]])\n",
      "tensor([[nan, nan]]) tensor([[-0.2370, -0.1255]])\n",
      "tensor([[nan, nan]]) tensor([[ 0.5805, -0.6962]])\n",
      "tensor([[nan, nan]]) tensor([[ 0.4379, -1.7237]])\n",
      "tensor([[nan, nan]]) tensor([[-0.6319,  0.4150]])\n",
      "tensor([[nan, nan]]) tensor([[-0.3518,  0.2606]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[-2.5366,  0.4207]])\n",
      "tensor([[nan, nan]]) tensor([[-1.8059,  0.0741]])\n",
      "tensor([[nan, nan]]) tensor([[-4.1829,  0.6795]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[-0.7489,  1.8875]])\n",
      "tensor([[nan, nan]]) tensor([[-0.3900,  0.2434]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[ 0.7109, -0.4845]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[-1.1169, -5.4220]])\n",
      "tensor([[nan, nan]]) tensor([[-0.7031, -4.8883]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[-2.8939,  1.1486]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[0.2804, 2.9767]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[ 0.2911, -4.1461]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[ 1.1545, -0.7550]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[-0.9682,  0.9449]])\n",
      "tensor([[nan, nan]]) tensor([[ 0.4237, -0.8840]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[-1.4113,  0.4112]])\n",
      "tensor([[nan, nan]]) tensor([[15.7917, -0.5818]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[ 0.3157, -0.9713]])\n",
      "tensor([[nan, nan]]) tensor([[ 0.4687, 11.9813]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[ 3.1335, -2.0369]])\n",
      "tensor([[nan, nan]]) tensor([[-0.8398,  1.1908]])\n",
      "tensor([[nan, nan]]) tensor([[ 0.2403, -0.5353]])\n",
      "tensor([[nan, nan]]) tensor([[ -0.1299, -12.4368]])\n",
      "tensor([[nan, nan]]) tensor([[ 2.8791, -1.8692]])\n",
      "tensor([[nan, nan]]) tensor([[277.0057,  -0.2954]])\n",
      "tensor([[nan, nan]]) tensor([[-0.2624,  0.6311]])\n",
      "tensor([[nan, nan]]) tensor([[2.2268, 0.6785]])\n",
      "tensor([[nan, nan]]) tensor([[  1.8774, -41.7189]])\n",
      "tensor([[nan, nan]]) tensor([[ 1.4568, -5.5817]])\n",
      "tensor([[nan, nan]]) tensor([[-1.9010,  0.1799]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[ 1.4268, -0.9739]])\n",
      "tensor([[nan, nan]]) tensor([[ 0.2354, -2.9494]])\n",
      "tensor([[nan, nan]]) tensor([[3.7747, 0.1951]])\n",
      "tensor([[nan, nan]]) tensor([[ 4.2002, -0.6980]])\n",
      "tensor([[nan, nan]]) tensor([[ 0.2841, -0.2767]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[-0.3122,  0.5483]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[-1.2668,  0.4867]])\n",
      "tensor([[nan, nan]]) tensor([[ 0.0353, -3.1456]])\n",
      "tensor([[nan, nan]]) tensor([[-0.9097,  1.2388]])\n",
      "tensor([[nan, nan]]) tensor([[-1.5012,  3.6653]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[ 0.2764, -1.5919]])\n",
      "tensor([[nan, nan]]) tensor([[-25.2031,   0.0289]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[ 2.5267, -0.4320]])\n",
      "tensor([[nan, nan]]) tensor([[12.1140,  1.0121]])\n",
      "tensor([[nan, nan]]) tensor([[-2.4887,  1.1018]])\n",
      "tensor([[nan, nan]]) tensor([[6.9492, 0.0572]])\n",
      "tensor([[nan, nan]]) tensor([[-0.5434, -1.7283]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[-13.3193,  -0.4878]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[ 7.4594, -2.1188]])\n",
      "tensor([[nan, nan]]) tensor([[-1.2017,  2.5874]])\n",
      "tensor([[nan, nan]]) tensor([[ 1.7441, -0.0076]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[ 2.0681, -0.5880]])\n",
      "tensor([[nan, nan]]) tensor([[ 0.8318, -2.8633]])\n",
      "tensor([[nan, nan]]) tensor([[ 2.3005, -0.1530]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[-0.4337,  1.3537]])\n",
      "tensor([[nan, nan]]) tensor([[ 2.2206, -0.1007]])\n",
      "tensor([[nan, nan]]) tensor([[-0.9478,  0.8356]])\n",
      "tensor([[nan, nan]]) tensor([[-1.9173,  2.0977]])\n",
      "tensor([[nan, nan]]) tensor([[ 1.6893, -0.9100]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[ 1.9612, -1.5275]])\n",
      "tensor([[nan, nan]]) tensor([[-0.3977,  0.1704]])\n",
      "tensor([[nan, nan]]) tensor([[ 0.9912, -1.1981]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[ 0.3694, -3.0150]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[-3.3608,  2.3311]])\n",
      "tensor([[nan, nan]]) tensor([[0.8142, 3.4998]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[-1.2627,  2.4285]])\n",
      "tensor([[nan, nan]]) tensor([[-0.0454, -5.6941]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[-1.5360,  0.5215]])\n",
      "tensor([[nan, nan]]) tensor([[ 0.6384, -0.7825]])\n",
      "tensor([[nan, nan]]) tensor([[-1.2880,  0.1569]])\n",
      "tensor([[nan, nan]]) tensor([[ -0.6156, -25.4950]])\n",
      "tensor([[nan, nan]]) tensor([[ -0.7735, -42.1218]])\n",
      "tensor([[nan, nan]]) tensor([[-3.7006,  0.0894]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[ 5.7864, -0.9429]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[ 1.0393, -0.3746]])\n",
      "tensor([[nan, nan]]) tensor([[-1.3676,  3.2138]])\n",
      "tensor([[nan, nan]]) tensor([[-3.4568, -1.3161]])\n",
      "tensor([[nan, nan]]) tensor([[ 1.5092, -0.5271]])\n",
      "tensor([[nan, nan]]) tensor([[nan, nan]])\n",
      "tensor([[nan, nan]]) tensor([[-1.7352,  1.7346]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        predicted = outputs\n",
    "        print(predicted, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f08154012ddadd8e950e6e9e035c7a7b32c136e7647e9b7c77e02eb723a8bedb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
